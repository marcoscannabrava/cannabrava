{"componentChunkName":"component---src-templates-blog-post-js","path":"/slm/","result":{"data":{"site":{"siteMetadata":{"title":"Marcos Cannabrava"}},"markdownRemark":{"id":"5699610d-359e-5be4-8602-61c4eece42be","excerpt":"why Efficient to deploy, fast, and actually really good! For people who can actually architect systems of agents, it's striking me more and more as a better…","html":"<h4>why</h4>\n<p>Efficient to deploy, fast, and actually really good!</p>\n<p>For people who can actually architect systems of agents, it's striking me more and more as a better alternative to calling the heavy-weights when you can get more targeted, faster, context-aware predictions with smaller and specialist models. My latest explorations with DeepSeek Coder 8B and Phi 3 left me with a really good impression of the current state of the short kings.</p>\n<p>Some reasons I'm excited.</p>\n<ol>\n<li><strong>modular development and simplified audits</strong></li>\n</ol>\n<p>You can actually apply system design thinking and you're not betting your business on some miracle moody API. The overreliance on these APIs for literally anything these days reminds me of <a href=\"https://en.wikipedia.org/wiki/God_object\">God objects</a>. The smaller size of SLMs also lowers the barrier for conducting audits, verification, and customization to meet regulations. It’s easier to understand how the model processes data, and implement your own encryption or logging.</p>\n<ol start=\"2\">\n<li><strong>running on isolated and low-end hardware</strong></li>\n</ol>\n<p>SLMs can operate almost anywhere: from a local server in a private network to a doctor’s or inspector’s device. The edge is here. Robots, cars, drones.</p>\n<ol start=\"3\">\n<li><strong>distributed security architecture</strong></li>\n</ol>\n<p>Unlike the monolithic architecture of LLMs, where all security components are “baked” into one large model, SLMs enable the creation of a distributed security system.</p>\n<ol start=\"4\">\n<li><strong>overindexing on llms can lead to unexpected results</strong></li>\n</ol>\n<p>They're non-deterministic machines after all, and the larger the task we give them, the worse they perform. Add to that the current providers are not reliable at all. Check OpenAI's or Anthropic's status pages and not a week goes by without downtime.</p>\n<ol start=\"5\">\n<li><strong>money $$$</strong></li>\n</ol>\n<p>This definitely should not be last but, in the age of VC-fueled bonanza we live in, it's easy to forget. Most AI startups' business models simply will not survive LLM unit-economics long-term. The bet is on some massive efficiency improvements, which although likely to some degree, might not happen in the scale these businesses expect. Let's not forget <a href=\"https://x.com/sama/status/1876104315296968813\">OpenAI itself is losing money on their most expensive plan</a> and Uber is still not profitable after 15 years. At some point the bill arrives. Meanwhile connect a high-pressure pipe from your wallets straight to NVIDIA's headquarters.</p>\n<h4>how</h4>\n<p>Setting up a runtime like <a href=\"https://vllm.readthedocs.io/\">vLLM</a> on <a href=\"https://aws.amazon.com/ai/machine-learning/inferentia/\">AWS INF2</a> has shown some promising results in my testing. I absolutely love the work AWS is doing with the Inferentia chips. More realistically, for production workloads, you'd get started with an offering from Google Vertex AI or AWS Bedrock suite of products. Deploying them on the edge can be done with Microsoft's <a href=\"https://onnxruntime.ai/\">ONNX</a> or even <a href=\"https://wasmedge.org/docs/category/ai-inference/\">WASM</a>.</p>\n<h4>what</h4>\n<p>Probably* you are not going to be doing image/video-generation with those models but CV, NLP, and function calling are all doable.\n<br/>*<em>I don't know honestly, maybe soon.</em></p>\n<p>Some real-life examples:</p>\n<ul>\n<li>Summarizing domain-specific documents like regulations. <em>#phi-3.5</em></li>\n<li>Smart summarization where you split documents in logical sections instead of dumping everything in a mega prompt. <em>#phi-3.5</em></li>\n<li>Generating marketing collateral, snippets, personalized customer support responses. <em>#phi-3.5</em></li>\n<li>Digitization of images and handwritten text. <em>#MiniCPM-Llama3-V2.5</em></li>\n<li>Data extraction for both structured and unstructured data. <em>#MiniCPM-Llama3-V2.5</em></li>\n<li>Content Moderation <em>#LLaMA3.18B</em></li>\n<li>Retail demand forecasting <em>train your own with AutoML</em></li>\n<li>Helpdesk support and routing <em>#LLaMA3.18B</em></li>\n<li>Diabetes tests! <em>#Diabetica-7B</em></li>\n</ul>\n<h4>interesting</h4>\n<ul>\n<li><a href=\"https://www.nvidia.com/en-us/ai/\">NVIDIA NIM for Generative AI</a></li>\n<li><a href=\"https://lmsys.org/projects/\">Projects | LMSYS Org</a></li>\n<li><a href=\"https://encord.com/blog/top-multimodal-models/\">How Multimodal Models Work and Top 10 Multimodal Models</a></li>\n<li><a href=\"https://hatchworks.com/blog/gen-ai/small-language-models/\">Building your own SLMs</a></li>\n<li><a href=\"https://deviniti.com/blog/enterprise-software/small-language-models-for-enterprise-ai/\">Small Language Models for enterprise AI: Benefits and deployment | Deviniti</a></li>\n</ul>\n<h4>models</h4>\n<ul>\n<li>\n<p>Vision tasks</p>\n<ul>\n<li><a href=\"https://huggingface.co/openbmb/MiniCPM-Llama3-V-2_5\">openbmb/MiniCPM-Llama3-V-2_5</a></li>\n<li><a href=\"https://huggingface.co/microsoft/Phi-3-vision-128k-instruct\">microsoft/Phi-3-vision-128k-instruct</a></li>\n<li><a href=\"https://huggingface.co/OpenGVLab/InternVL2_5-8B-MPO\">OpenGVLab/InternVL2_5-8B-MPO</a></li>\n</ul>\n</li>\n<li>\n<p>Qwen family doing great</p>\n<ul>\n<li><a href=\"https://huggingface.co/Qwen/Qwen2-Audio-7B-Instruct\">Qwen2-Audio-7B-Instruct</a></li>\n<li><a href=\"https://huggingface.co/Qwen/Qwen2-VL-7B-Instruct\">Qwen2-VL-7B-Instruct</a></li>\n<li><a href=\"https://huggingface.co/Qwen/Qwen2.5-7B-Instruct-GPTQ-Int4\">Qwen2.5-7B-Instruct-GPTQ-Int4</a></li>\n</ul>\n</li>\n<li>\n<p>Optimized for function calling:</p>\n<ul>\n<li><a href=\"https://huggingface.co/watt-ai/watt-tool-8B\">watt-ai/watt-tool-8B</a> — <em>haven't tried it but I'm curious that Berkeley is saying this tiny unknown model is head-to-head with GPT 4o</em></li>\n</ul>\n</li>\n<li>\n<p>General NLP</p>\n<ul>\n<li><a href=\"https://huggingface.co/microsoft/Phi-3.5-mini-instruct-onnx\">microsoft/Phi-3.5-mini-instruct-onnx</a></li>\n<li><a href=\"https://huggingface.co/meta-llama/Llama-3.1-8B\">Llama-3.1-8B</a></li>\n</ul>\n</li>\n</ul>\n<p>I only put open-source models above but Gemini Flash reserves an honorable mention.</p>\n<h4>leaderboards</h4>\n<ul>\n<li><a href=\"https://lmarena.ai/\">Chatbot Arena (LMSys)</a></li>\n<li><a href=\"https://huggingface.co/opencompass\">opencompass)</a></li>\n<li><a href=\"https://www.vellum.ai/llm-leaderboard\">LLM Leaderboard 2024</a></li>\n<li><a href=\"https://scale.com/leaderboard\">SEAL LLM Leaderboards: Expert-Driven Private Evaluations</a></li>\n</ul>\n<h4>resources</h4>\n<ul>\n<li><a href=\"https://www.superannotate.com/blog/small-language-models\">Small Language Models (SLMs) [2024 overview] | SuperAnnotate</a></li>\n<li><a href=\"https://towardsdatascience.com/your-company-needs-small-language-models-d0a223e0b6d9\">Your Company Needs Small Language Models - When specialized models outperform general-purpose models (Sergei Savvov)</a></li>\n<li><a href=\"https://www.datacamp.com/blog/top-small-language-models\">Top 15 Small Language Models for 2024</a></li>\n<li><a href=\"https://www.symphonyai.com/glossary/ai/slm-or-smlm-small-language-model/\">Small Language Model (SLM or SMLM)</a></li>\n</ul>\n<h4>papers</h4>\n<ul>\n<li><a href=\"https://arxiv.org/abs/2410.20011\">A Survey of Small Language Models</a></li>\n<li><a href=\"https://arxiv.org/abs/2411.03350\">A Comprehensive Survey of Small Language Models in the Era of Large Language Models: Techniques, Enhancements, Applications, Collaboration with LLMs, and Trustworthiness</a></li>\n</ul>","frontmatter":{"title":"Small Language Models (SLMs)","date":"January 07, 2025","description":"exploring models that carry more than their weight"}},"previous":{"fields":{"slug":"/philosophy/stoicism/"},"frontmatter":{"title":"Stoicism"}},"next":null},"pageContext":{"id":"5699610d-359e-5be4-8602-61c4eece42be","previousPostId":"589280bd-3406-5e0e-bca5-f67d3534f2a1","nextPostId":null}},"staticQueryHashes":["2525930961","3728613372"],"slicesMap":{}}